# Guide d'int√©gration iOS - Classification Zero-Shot mDeBERTa

## üì± Setup complet pour iOS

### √âtape 1: Cr√©er un nouveau projet Xcode

1. Ouvrez Xcode
2. File > New > Project
3. Choisissez "App" (iOS)
4. Nommez votre projet (ex: "TextClassifierApp")

### √âtape 2: Ajouter les d√©pendances

#### Option A: CocoaPods (Recommand√©)

1. Cr√©ez un `Podfile` √† la racine du projet:

```ruby
platform :ios, '15.0'
use_frameworks!

target 'TextClassifierApp' do
  pod 'onnxruntime-objc', '~> 1.19.2'
end
```

2. Installez les pods:

```bash
pod install
```

3. Ouvrez le workspace:

```bash
open TextClassifierApp.xcworkspace
```

#### Option B: Swift Package Manager

1. Xcode > File > Add Package Dependencies
2. Ajoutez: `https://github.com/microsoft/onnxruntime-swift-package-manager`
3. Version: 1.19.2 ou sup√©rieure

### √âtape 3: Ajouter les fichiers du mod√®le

1. **T√©l√©charger les fichiers n√©cessaires:**
   - `model_quantized.onnx` (338 MB)
   - `spm.model` (tokenizer SentencePiece)

2. **Ajouter au projet:**
   - Glissez-d√©posez les fichiers dans Xcode
   - Cochez "Copy items if needed"
   - Cochez votre Target
   - V√©rifiez dans Build Phases > Copy Bundle Resources

### √âtape 4: Impl√©menter le tokenizer SentencePiece

Le mod√®le utilise SentencePiece pour la tokenisation. Vous avez deux options:

#### Option A: Wrapper Python (d√©veloppement/test)

Pour le d√©veloppement rapide, utilisez PythonKit:

```swift
import PythonKit

class SentencePieceTokenizer {
    let tokenizer: PythonObject

    init(modelPath: String) throws {
        let transformers = Python.import("transformers")
        tokenizer = transformers.AutoTokenizer.from_pretrained(modelPath)
    }

    func encode(text: String, textPair: String?, maxLength: Int) -> ([Int32], [Int32]) {
        let inputs = tokenizer(
            text,
            textPair,
            truncation: true,
            max_length: maxLength,
            padding: "max_length",
            return_tensors: "np"
        )

        let inputIds = Array<Int32>(numpy: inputs["input_ids"])!
        let attentionMask = Array<Int32>(numpy: inputs["attention_mask"])!

        return (inputIds, attentionMask)
    }
}
```

#### Option B: Biblioth√®que native (production)

Pour la production, utilisez une impl√©mentation native:

```ruby
# Podfile
pod 'SentencePiece'
```

```swift
import SentencePiece

class SentencePieceTokenizer {
    private let processor: SPMProcessor

    init(modelPath: String) throws {
        processor = try SPMProcessor(modelPath: modelPath)
    }

    func encode(text: String, textPair: String?, maxLength: Int) -> ([Int32], [Int32]) {
        // Combiner text et textPair avec le s√©parateur [SEP]
        let fullText = textPair != nil ? "\\(text) [SEP] \\(textPair!)" : text

        // Tokeniser
        var tokens = processor.encode(fullText)

        // Tronquer ou padder √† maxLength
        if tokens.count > maxLength {
            tokens = Array(tokens.prefix(maxLength))
        }

        let inputIds = tokens.map { Int32($0) }
        let attentionMask = Array(repeating: Int32(1), count: tokens.count)

        // Padding si n√©cessaire
        let paddingLength = maxLength - tokens.count
        if paddingLength > 0 {
            let paddedInputIds = inputIds + Array(repeating: Int32(0), count: paddingLength)
            let paddedMask = attentionMask + Array(repeating: Int32(0), count: paddingLength)
            return (paddedInputIds, paddedMask)
        }

        return (inputIds, attentionMask)
    }
}
```

### √âtape 5: Int√©grer TextClassifier

Copiez `TextClassifier.swift` dans votre projet et utilisez-le:

```swift
import SwiftUI

struct ContentView: View {
    @State private var text = ""
    @State private var results: [TextClassifier.ClassificationResult] = []
    @State private var isLoading = false

    private let classifier = TextClassifier()
    private let labels = ["politics", "economy", "technology", "sports", "entertainment"]

    var body: some View {
        VStack(spacing: 20) {
            Text("Classification Zero-Shot")
                .font(.title)
                .bold()

            TextEditor(text: $text)
                .frame(height: 150)
                .border(Color.gray, width: 1)
                .padding()

            Button("Classifier") {
                classifyText()
            }
            .disabled(text.isEmpty || isLoading)
            .buttonStyle(.borderedProminent)

            if isLoading {
                ProgressView()
            }

            List(results, id: \\.label) { result in
                HStack {
                    Text(result.label)
                        .font(.headline)
                    Spacer()
                    Text(String(format: "%.1f%%", result.score * 100))
                        .font(.subheadline)
                        .foregroundColor(.secondary)

                    // Barre de progression
                    GeometryReader { geometry in
                        ZStack(alignment: .leading) {
                            Rectangle()
                                .frame(width: geometry.size.width, height: 8)
                                .opacity(0.3)
                                .foregroundColor(.gray)

                            Rectangle()
                                .frame(width: geometry.size.width * CGFloat(result.score), height: 8)
                                .foregroundColor(.blue)
                        }
                    }
                    .frame(height: 8)
                }
            }
        }
        .padding()
        .onAppear {
            loadModel()
        }
    }

    private func loadModel() {
        guard let modelPath = Bundle.main.path(forResource: "model_quantized", ofType: "onnx"),
              let tokenizerPath = Bundle.main.path(forResource: "spm", ofType: "model") else {
            print("‚ùå Fichiers du mod√®le introuvables")
            return
        }

        do {
            try classifier.loadModel(modelPath: modelPath, tokenizerPath: tokenizerPath)
        } catch {
            print("‚ùå Erreur lors du chargement: \\(error)")
        }
    }

    private func classifyText() {
        isLoading = true

        Task {
            do {
                let classificationResults = try classifier.classify(
                    text: text,
                    candidateLabels: labels
                )

                await MainActor.run {
                    results = classificationResults
                    isLoading = false
                }
            } catch {
                print("‚ùå Erreur: \\(error)")
                await MainActor.run {
                    isLoading = false
                }
            }
        }
    }
}
```

### √âtape 6: Configuration de la m√©moire

Le mod√®le est volumineux. Dans `Info.plist`, ajoutez:

```xml
<key>UIApplicationExitsOnSuspend</key>
<false/>
<key>UIBackgroundModes</key>
<array>
    <string>processing</string>
</array>
```

### √âtape 7: Optimisations pour la performance

#### Mise en cache du mod√®le

```swift
class ModelManager {
    static let shared = ModelManager()
    private(set) var classifier: TextClassifier?

    private init() {}

    func loadModel() throws {
        guard classifier == nil else { return }

        let classifier = TextClassifier()
        try classifier.loadModel(/* ... */)
        self.classifier = classifier
    }
}
```

#### Inf√©rence en arri√®re-plan

```swift
func classify(text: String) async throws -> [ClassificationResult] {
    try await Task.detached(priority: .userInitiated) {
        try classifier.classify(text: text, candidateLabels: labels)
    }.value
}
```

### √âtape 8: Tester sur device

1. Connectez un iPhone/iPad
2. S√©lectionnez votre device dans Xcode
3. Cmd + R pour compiler et lancer
4. Testez avec diff√©rents textes

## üêõ Probl√®mes courants

### "Model file too large"

Le mod√®le quantifi√© (338 MB) peut d√©passer les limites de certaines configurations. Solutions:

1. T√©l√©chargez le mod√®le au premier lancement:

```swift
func downloadModelIfNeeded() async throws {
    let modelURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        .appendingPathComponent("model_quantized.onnx")

    if !FileManager.default.fileExists(atPath: modelURL.path) {
        // T√©l√©charger depuis votre serveur
        let remoteURL = URL(string: "https://your-server.com/model_quantized.onnx")!
        let (localURL, _) = try await URLSession.shared.download(from: remoteURL)
        try FileManager.default.moveItem(at: localURL, to: modelURL)
    }
}
```

2. H√©bergez le mod√®le sur un serveur et faites des appels API

### "Out of memory"

1. Fermez les autres apps
2. R√©duisez `maxSequenceLength` de 128 √† 64
3. Utilisez le mod√®le sur iPhone r√©cents uniquement (‚â• iPhone 12)

### "Slow inference"

1. Utilisez `model_quantized.onnx` (pas `model.onnx`)
2. Activez CoreML dans ONNX Runtime:

```swift
let options = try ORTSessionOptions()
options.appendExecutionProvider(.coreML)
```

3. R√©duisez la longueur de s√©quence

## üìä Performance attendue

| Device        | Temps d'inf√©rence | M√©moire |
|---------------|-------------------|---------|
| iPhone 15 Pro | ~80ms            | 450 MB  |
| iPhone 14     | ~120ms           | 480 MB  |
| iPhone 12     | ~180ms           | 520 MB  |
| iPhone 11     | ~250ms           | 580 MB  |

## üéØ Prochaines √©tapes

- [ ] Impl√©menter le cache des r√©sultats
- [ ] Ajouter la d√©tection de langue
- [ ] Cr√©er une UI plus √©l√©gante
- [ ] Tests unitaires
- [ ] Tests d'int√©gration

## üìö Ressources

- [ONNX Runtime iOS](https://onnxruntime.ai/docs/tutorials/mobile/ios.html)
- [SentencePiece iOS](https://github.com/google/sentencepiece)
- [Mod√®le HuggingFace](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)
